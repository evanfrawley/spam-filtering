{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"should've\", 's', 'at', 'in', 'above', 'ma', 'm', 'should', 'didn', 'yours', 'the', \"hasn't\", 'did', 'needn', 'we', 'those', \"you'd\", 'hers', 'won', 'ours', 'mustn', 'was', \"you'll\", 'isn', 'she', 'and', 'if', 'you', 'herself', \"weren't\", 'what', 'who', 'any', 'haven', 'mightn', 'down', 'each', 'further', 'but', 'most', 'yourselves', 'his', 't', 'between', 'again', 'ourselves', 'an', 'other', \"hadn't\", \"it's\", \"aren't\", 'these', 'shan', 'some', 'y', 'have', 'were', 'by', 'he', 'had', 'until', 'of', 'nor', 'himself', \"you've\", 'their', \"didn't\", 'myself', 'that', 'your', 'here', 'does', 'under', \"isn't\", \"mightn't\", 'up', 'themselves', 'where', 'how', 'shouldn', 'now', \"doesn't\", 'be', 'has', 'hasn', 'no', 'will', 'when', 'doing', 'below', 'or', 'for', 'there', 'from', 'then', 'me', 'whom', \"she's\", 'ain', 'why', \"won't\", 'once', \"wasn't\", 'with', 'own', 'before', 'd', 'o', 'a', \"that'll\", 'both', 'am', 'are', 'while', 'this', 'wouldn', 'weren', 'on', 'its', 'itself', 've', 'off', 'over', 'her', 'so', 'during', 'can', 'him', 'do', 're', 'is', 'than', 'them', 'not', \"wouldn't\", 'theirs', 'having', 'i', 'all', 'about', 'into', 'through', 'after', 'been', 'couldn', \"mustn't\", \"shan't\", 'aren', 'more', \"don't\", \"shouldn't\", 'll', 'too', 'my', \"haven't\", 'same', 'against', 'which', 'wasn', 'few', 'only', 'very', 'just', 'being', 'out', \"needn't\", 'because', 'as', 'hadn', 'yourself', 'they', 'to', 'such', 'our', 'it', 'don', \"couldn't\", \"you're\", 'doesn'}\n"
     ]
    }
   ],
   "source": [
    "import mailbox\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "def createCSV(name, mbox, mes_type):\n",
    "    #createing SPAM csv\n",
    "    writer = csv.writer(open(name, \"w\", encoding=\"utf-8\"))\n",
    "    writer.writerow(['Subject', 'From', 'Date', 'Message-ID', 'Payload', 'Type'])\n",
    "    for message in mailbox.mbox(mbox):\n",
    "\n",
    "        if message.is_multipart():\n",
    "            content = ''.join(str(part.get_payload(decode=True)) for part in message.get_payload())\n",
    "            content = cleanMe(content)\n",
    "        else:\n",
    "            content = str(message.get_payload(decode=True))\n",
    "            content = cleanMe(content)\n",
    "#         content = content.split()\n",
    "        #print(content)\n",
    "        writer.writerow([message['subject'], message['from'], message['date'], message['Message-Id'], content, mes_type])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMe(html):\n",
    "    soup = bs(html,\"lxml\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n '.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\t', ' ').replace(\"\\n\", ' ').replace('\\\\b', ' ')\n",
    "    # gets rid of escape characters\n",
    "    text = bytes(text, \"utf-8\").decode(\"unicode_escape\")\n",
    "    # gets rid of links \n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = \" \".join(re.findall(r'\\s+[a-zA-Z]+\\s+', text))\n",
    "    # make text lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove stop words\n",
    "    resultwords  = [word for word in text.split() if word not in stop_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#createCSV('./data/spam_no_stop.csv', './data/Spam.mbox', 'Spam')\n",
    "#createCSV('./data/inbox_no_stop.csv', './data/Inbox.mbox', 'Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707, 6)\n",
      "51112\n"
     ]
    }
   ],
   "source": [
    "spam1 = pd.read_csv(\"./data/spam1.csv\")\n",
    "print(spam1.shape)\n",
    "inbox1 = pd.read_csv(\"./data/inbox.csv\")\n",
    "print(inbox1.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Time taken:  0.10048580169677734\n",
      "Ham Time taken:  6.080684661865234\n"
     ]
    }
   ],
   "source": [
    "# inbox concatenated vectors = icv\n",
    "icv_id = []\n",
    "icv_word = []\n",
    "icv_target = []\n",
    "\n",
    "spam_start = time.time()\n",
    "for index, row in spam1.iterrows():\n",
    "    email_words = str(row['Payload']).strip()\n",
    "    unique_id = row[\"Message-ID\"]\n",
    "    words_dict = {}\n",
    "    words_vec = email_words.split(\" \")\n",
    "    for word in words_vec:\n",
    "        words_dict[word] = True\n",
    "    for word in words_dict.keys():\n",
    "        icv_id.append(unique_id)\n",
    "        icv_word.append(word)\n",
    "        icv_target.append(1)\n",
    "\n",
    "print(\"Spam Time taken: \", str(time.time() - spam_start))\n",
    "\n",
    "inbox_start = time.time()\n",
    "for index, row in inbox1.iterrows():\n",
    "    email_words = str(row['Payload']).strip()\n",
    "    unique_id = row[\"Message-ID\"]\n",
    "    words_dict = {}\n",
    "    words_vec = email_words.split(\" \")\n",
    "    for word in words_vec:\n",
    "        words_dict[word] = True\n",
    "    for word in words_dict.keys():\n",
    "        icv_id.append(unique_id)\n",
    "        icv_word.append(word)\n",
    "        icv_target.append(0)\n",
    "\n",
    "print(\"Ham Time taken: \", str(time.time() - inbox_start))\n",
    "            \n",
    "new_df = pd.DataFrame({\"id\":icv_id, \"word\": icv_word, \"target\": icv_target})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3850316, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_spam_df = new_df\n",
    "word_spam_df_features = word_spam_df.loc[:, \"word\"]\n",
    "\n",
    "word_spam_df_outcomes = word_spam_df.loc[:, \"target\"]\n",
    "word_spam_df.shape\n",
    "word_spam_df\n",
    "# tyring to take out stop words after df is made. didn't do \n",
    "df = word_spam_df[word_spam_df.word != stop_words]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013643644223161389 0.9863563557768386\n"
     ]
    }
   ],
   "source": [
    "total_ham = inbox1.shape[0]\n",
    "total_spam = spam1.shape[0]\n",
    "total = total_spam + total_ham\n",
    "p_ham = total_spam / total\n",
    "p_spam = total_ham / total\n",
    "print(p_ham, p_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maryh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\maryh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a3cce2893525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetPriors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_spam_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_ham\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_spam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_ham\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_spam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-a3cce2893525>\u001b[0m in \u001b[0;36mgetPriors\u001b[1;34m(word_and_spam, ham_total, spam_total, ham_p, spam_p)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseen_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mseen_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mw_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_and_spam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_and_spam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mspam_word_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_r\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_and_spam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mham_word_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_r\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_and_spam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getPriors(word_and_spam, ham_total, spam_total, ham_p, spam_p):\n",
    "    seen_words = {}\n",
    "    word_probs = []\n",
    "    # these could also perhaps be total spam or ham emails?\n",
    "    for index, row in word_and_spam.iterrows():\n",
    "        word = row['word']\n",
    "        if (word not in seen_words):\n",
    "            seen_words[word] = True\n",
    "            w_r = word_and_spam[word_and_spam[\"word\"] == word]\n",
    "            spam_word_count = len(w_r[(word_and_spam[\"target\"] == 1)])\n",
    "            ham_word_count = len(w_r[(word_and_spam[\"target\"] == 0)])\n",
    "            p_spam_word = spam_word_count / spam_total\n",
    "            p_ham_word = ham_word_count / ham_total\n",
    "            p_s = p_spam_word * spam_p\n",
    "            p = (p_s)  / (p_s + p_ham_word * ham_p)\n",
    "            word_probs.append({\"key\": word, \"val\": p})\n",
    "    return word_probs\n",
    "\n",
    "    \n",
    "print(getPriors(word_spam_df, total_ham, total_spam, p_ham, p_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# didn't get time to write this...but I think the getProirs dictionary could get changed to a matrix easily\n",
    "#matrixPrior(words_spam):\n",
    "#    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_spam_df_features, word_spam_df_outcomes, random_state=12345, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'how'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7c4e32f331f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnb_model_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnb_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m    185\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'how'"
     ]
    }
   ],
   "source": [
    "nb_model_1 = nb.GaussianNB().fit(X_train, y_train)\n",
    "nb_model_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for pipeline\n",
    "\n",
    "# random forest\n",
    "rfc = RandomForestClassifier()\n",
    "param_grid_rfc = {'randomforestclassifier__n_estimators': np.arange(1,10)}\n",
    "\n",
    "# niave bayse\n",
    "clf = GaussianNB()\n",
    "param_grid_rfc = {'gaussiannb__prior': True, False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, param_grid, xtrain, ytrain, do_pca = False):\n",
    "    scaler = MinMaxScaler()\n",
    "    if(do_pca == True):\n",
    "        pca = PCA(n_components = 10)\n",
    "        pipe = make_pipeline(pca, model)\n",
    "    else:\n",
    "        pipe = make_pipeline(model)\n",
    "    grid = GridSearchCV(pipe,param_grid)\n",
    "    grid.fit(xtrain, ytrain)\n",
    "    grid.best_params_\n",
    "    accuracy = grid.score(xtrain, ytrain)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

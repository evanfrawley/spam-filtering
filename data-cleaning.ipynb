{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then', 'over', 'having', 'just', 'theirs', 'before', 'those', 'now', 'that', 'she', 'll', 'ain', 'whom', \"hadn't\", 'yourselves', 'for', 'again', 'themselves', 'didn', 'with', 've', \"should've\", 'are', 'these', 'which', 'here', \"shouldn't\", 'as', 'your', 'a', 'not', 'doesn', 'but', 'into', 'wouldn', 'both', 'i', 'did', 'further', 'does', 'each', 'very', \"hasn't\", \"it's\", 'weren', 'was', 'can', \"wasn't\", 'd', 'you', \"you've\", 'our', 'couldn', 'this', 'am', 'yourself', 'too', 'aren', \"haven't\", 'don', 'yours', 'himself', \"weren't\", 'against', 'once', 'hasn', \"mustn't\", 'if', 'up', 're', 'under', \"isn't\", 'while', 'from', 'have', 'than', 'me', 'were', 'more', 'same', 'of', 'so', 'own', 'any', 'm', 'should', 'shouldn', 'hers', 'how', 'what', 'off', 'why', \"you'd\", 'about', 'be', 'my', 'an', 'to', 'needn', 'itself', \"doesn't\", 'the', 'or', 'after', 'ourselves', 's', 'during', 'between', 'until', 'his', 'there', 'few', 't', 'on', 'and', 'other', 'above', 'do', 'down', \"don't\", 'where', 'they', 'all', 'them', 'wasn', 'by', 'at', 'shan', 'their', 'doing', 'such', \"didn't\", 'who', 'had', 'will', 'myself', 'only', 'isn', 'below', 'it', 'him', 'is', 'when', 'hadn', 'nor', 'herself', 'because', 'most', 'haven', 'ma', 'out', \"couldn't\", 'mightn', 'its', \"you're\", 'her', \"won't\", 'ours', 'through', 'being', \"you'll\", 'no', \"needn't\", \"shan't\", 'won', \"she's\", 'o', 'some', 'mustn', 'we', \"wouldn't\", 'been', \"that'll\", 'he', 'in', 'y', \"mightn't\", \"aren't\", 'has'}\n"
     ]
    }
   ],
   "source": [
    "import mailbox\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "def createCSV(name, mbox, mes_type):\n",
    "    #createing SPAM csv\n",
    "    writer = csv.writer(open(name, \"w\", encoding=\"utf-8\"))\n",
    "    writer.writerow(['Subject', 'From', 'Date', 'Message-ID', 'Payload', 'Type'])\n",
    "    for message in mailbox.mbox(mbox):\n",
    "\n",
    "        if message.is_multipart():\n",
    "            content = ''.join(str(part.get_payload(decode=True)) for part in message.get_payload())\n",
    "            content = cleanMe(content)\n",
    "        else:\n",
    "            content = str(message.get_payload(decode=True))\n",
    "            content = cleanMe(content)\n",
    "#         content = content.split()\n",
    "        #print(content)\n",
    "        writer.writerow([message['subject'], message['from'], message['date'], message['Message-Id'], content, mes_type])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanMe(html):\n",
    "    soup = bs(html,\"lxml\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n '.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\t', ' ').replace(\"\\n\", ' ').replace('\\\\b', ' ')\n",
    "    # gets rid of escape characters\n",
    "    text = bytes(text, \"utf-8\").decode(\"unicode_escape\")\n",
    "    # gets rid of links \n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = \" \".join(re.findall(r'\\s+[a-zA-Z]+\\s+', text))\n",
    "    # make text lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove stop words before csv was written\n",
    "    resultwords  = [word for word in text.split() if word not in stop_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#createCSV('./data/spam_no_stop.csv', './data/Spam.mbox', 'Spam')\n",
    "#createCSV('./data/inbox_no_stop.csv', './data/Inbox.mbox', 'Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707, 6)\n",
      "51112\n"
     ]
    }
   ],
   "source": [
    "spam1 = pd.read_csv(\"./data/spam1.csv\")\n",
    "print(spam1.shape)\n",
    "inbox1 = pd.read_csv(\"./data/inbox.csv\")\n",
    "print(inbox1.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Time taken:  0.1002049446105957\n",
      "Ham Time taken:  6.941816806793213\n"
     ]
    }
   ],
   "source": [
    "# inbox concatenated vectors = icv\n",
    "icv_id = []\n",
    "icv_word = []\n",
    "icv_target = []\n",
    "\n",
    "spam_start = time.time()\n",
    "for index, row in spam1.iterrows():\n",
    "    email_words = str(row['Payload']).strip()\n",
    "    unique_id = row[\"Message-ID\"]\n",
    "    words_dict = {}\n",
    "    words_vec = email_words.split(\" \")\n",
    "    for word in words_vec:\n",
    "        words_dict[word] = True\n",
    "    for word in words_dict.keys():\n",
    "        icv_id.append(unique_id)\n",
    "        icv_word.append(word)\n",
    "        icv_target.append(1)\n",
    "\n",
    "print(\"Spam Time taken: \", str(time.time() - spam_start))\n",
    "\n",
    "inbox_start = time.time()\n",
    "for index, row in inbox1.iterrows():\n",
    "    email_words = str(row['Payload']).strip()\n",
    "    unique_id = row[\"Message-ID\"]\n",
    "    words_dict = {}\n",
    "    words_vec = email_words.split(\" \")\n",
    "    for word in words_vec:\n",
    "        words_dict[word] = True\n",
    "    for word in words_dict.keys():\n",
    "        icv_id.append(unique_id)\n",
    "        icv_word.append(word)\n",
    "        icv_target.append(0)\n",
    "\n",
    "print(\"Ham Time taken: \", str(time.time() - inbox_start))\n",
    "            \n",
    "new_df = pd.DataFrame({\"id\":icv_id, \"word\": icv_word, \"target\": icv_target})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3850402, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_spam_df = new_df\n",
    "word_spam_df_features = word_spam_df.loc[:, \"word\"]\n",
    "\n",
    "word_spam_df_outcomes = word_spam_df.loc[:, \"target\"]\n",
    "word_spam_df.shape\n",
    "word_spam_df\n",
    "# tyring to take out stop words after df is made. didn't do \n",
    "df = word_spam_df[word_spam_df.word != stop_words]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013643644223161389 0.9863563557768386\n"
     ]
    }
   ],
   "source": [
    "total_ham = inbox1.shape[0]\n",
    "total_spam = spam1.shape[0]\n",
    "total = total_spam + total_ham\n",
    "p_ham = total_spam / total\n",
    "p_spam = total_ham / total\n",
    "print(p_ham, p_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWordCounts(word_and_spam, ham_total, spam_total, ham_p, spam_p):\n",
    "    seen_words = {}\n",
    "    word_probs = []\n",
    "    \n",
    "    unique_words = word_and_spam.word.unique()\n",
    "    unique_words_for_priors = []\n",
    "    unique_word_counts = {}\n",
    "    for w in unique_words:\n",
    "        unique_word_counts[w] = {\"s\":0, \"h\":0}\n",
    "       \n",
    "    for index, row in word_and_spam.iterrows():\n",
    "        word = row['word']\n",
    "        if (word not in seen_words):\n",
    "            if (row[\"target\"] == 1):\n",
    "                unique_word_counts[word][\"s\"] = unique_word_counts[word][\"s\"] + 1\n",
    "            else:\n",
    "                unique_word_counts[word][\"h\"] = unique_word_counts[word][\"h\"] + 1\n",
    "                \n",
    "#             w_r = word_and_spam[word_and_spam[\"word\"] == word]\n",
    "#             spam_word_count = len(w_r[(word_and_spam[\"target\"] == 1)])\n",
    "#             ham_word_count = len(w_r[(word_and_spam[\"target\"] == 0)])\n",
    "#             p_spam_word = spam_word_count / spam_total\n",
    "#             p_ham_word = ham_word_count / ham_total\n",
    "#             p_s = p_spam_word * spam_p\n",
    "#             p = (p_s)  / (p_s + p_ham_word * ham_p)\n",
    "#             word_probs.append({\"key\": word, \"val\": p})\n",
    "            \n",
    "#     spam_status_for_unique_words = []\n",
    "#     for w in unique_words:\n",
    "#         spam_status_for_unique_words.append(0)\n",
    "#         spam_status_for_unique_words.append(1)\n",
    "#         unique_words_for_priors.append(w)\n",
    "#         unique_words_for_priors.append(w)\n",
    "        \n",
    "    # these could also perhaps be total spam or ham emails?\n",
    "#     for index, row in word_and_spam.iterrows():\n",
    "#         word = row['word']\n",
    "#         if (word not in seen_words):\n",
    "#             seen_words[word] = True\n",
    "#             w_r = word_and_spam[word_and_spam[\"word\"] == word]\n",
    "#             spam_word_count = len(w_r[(word_and_spam[\"target\"] == 1)])\n",
    "#             ham_word_count = len(w_r[(word_and_spam[\"target\"] == 0)])\n",
    "#             p_spam_word = spam_word_count / spam_total\n",
    "#             p_ham_word = ham_word_count / ham_total\n",
    "#             p_s = p_spam_word * spam_p\n",
    "#             p = (p_s)  / (p_s + p_ham_word * ham_p)\n",
    "#             word_probs.append({\"key\": word, \"val\": p})\n",
    "    return unique_word_counts\n",
    "\n",
    "\n",
    "counts_df = getUniqueWordCounts(word_spam_df, total_ham, total_spam, p_ham, p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013643644223161389 0.9863563557768386\n"
     ]
    }
   ],
   "source": [
    "# counts_df_not_dict = pd.DataFrame(counts_df)\n",
    "total_ham = inbox1.shape[0]\n",
    "total_spam = spam1.shape[0]\n",
    "total = total_spam + total_ham\n",
    "p_ham = total_spam / total\n",
    "p_spam = total_ham / total\n",
    "print(p_ham, p_spam)\n",
    "\n",
    "word_arr = []\n",
    "target_arr = []\n",
    "p_arr = []\n",
    "for key, val in counts_df.items():\n",
    "    spam_word_count = val[\"s\"]\n",
    "    ham_word_count = val[\"h\"]\n",
    "    \n",
    "    p_word_given_spam = spam_word_count / total_spam\n",
    "    p_word_given_ham = ham_word_count / total_ham    \n",
    "    p_s = p_word_given_spam * p_spam\n",
    "    p_h = p_word_given_ham * p_ham\n",
    "    \n",
    "    p_spam_given_word = (p_s)  / (p_s + p_h)\n",
    "    p_ham_given_word = (p_h)  / (p_s + p_h)\n",
    "    \n",
    "    word_arr.append(key)\n",
    "    target_arr.append(1)\n",
    "    p_arr.append(p_spam_given_word)\n",
    "    word_arr.append(key)\n",
    "    target_arr.append(0)\n",
    "    p_arr.append(p_ham_given_word)\n",
    "\n",
    "    \n",
    "final_df = pd.DataFrame(data={\"word\":word_arr,\"target\":target_arr,\"p\":p_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"priors\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaarjlbasmw</th>\n",
       "      <th>aaaaaaabaaidbaugbwgjcgsqaaeeaqmcbaifbwyibqmmmweaahedbcesmqvbuwetingbmgyukagx</th>\n",
       "      <th>aaaaaaabaaidbaugbwgjcgsqaaibawmcbaigbwmeagyccwecaxeeaauhejfbuqytysjxgrqykaeh</th>\n",
       "      <th>aaaaaall</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaacoppertop</th>\n",
       "      <th>...</th>\n",
       "      <th>zzq</th>\n",
       "      <th>zzqj</th>\n",
       "      <th>zzqqwos</th>\n",
       "      <th>zzt</th>\n",
       "      <th>zzu</th>\n",
       "      <th>zzug</th>\n",
       "      <th>zzv</th>\n",
       "      <th>zzy</th>\n",
       "      <th>zzym</th>\n",
       "      <th>zzzup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>26196</td>\n",
       "      <td>143</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 64490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a   aa  aaa  \\\n",
       "h  26196  143   35   \n",
       "s    286    0    0   \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
       "h                                                  1                              \n",
       "s                                                  0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaarjlbasmw  \\\n",
       "h                                                  1                              \n",
       "s                                                  0                              \n",
       "\n",
       "   aaaaaaabaaidbaugbwgjcgsqaaeeaqmcbaifbwyibqmmmweaahedbcesmqvbuwetingbmgyukagx  \\\n",
       "h                                                  1                              \n",
       "s                                                  0                              \n",
       "\n",
       "   aaaaaaabaaidbaugbwgjcgsqaaibawmcbaigbwmeagyccwecaxeeaauhejfbuqytysjxgrqykaeh  \\\n",
       "h                                                  2                              \n",
       "s                                                  0                              \n",
       "\n",
       "   aaaaaall  aaaaand  aaacoppertop  ...    zzq  zzqj  zzqqwos  zzt  zzu  zzug  \\\n",
       "h         1        1             2  ...      3     1        1    2    2     2   \n",
       "s         0        0             0  ...      0     0        0    0    0     0   \n",
       "\n",
       "   zzv  zzy  zzym  zzzup  \n",
       "h    1    2     1      1  \n",
       "s    0    0     0      0  \n",
       "\n",
       "[2 rows x 64490 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df_not_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# didn't get time to write this...but I think the getProirs dictionary could get changed to a matrix easily\n",
    "#matrixPrior(words_spam):\n",
    "#    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_spam_df_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-168e7ec0e04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_spam_df_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_spam_df_outcomes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12345\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_spam_df_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_spam_df_features, word_spam_df_outcomes, random_state=12345, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7c4e32f331f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_model_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "nb_model_1 = nb.GaussianNB().fit(X_train, y_train)\n",
    "nb_model_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-23a6b5bfdeb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# niave bayse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mparam_grid_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'gaussiannb__prior'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters for pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest\n",
    "rfc = RandomForestClassifier()\n",
    "param_grid_rfc = {'randomforestclassifier__n_estimators': np.arange(1,10)}\n",
    "\n",
    "# niave bayse\n",
    "clf = GaussianNB()\n",
    "param_grid_rfc = {'gaussiannb__prior': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, param_grid, xtrain, ytrain, do_pca = False):\n",
    "    scaler = MinMaxScaler()\n",
    "    if(do_pca == True):\n",
    "        pca = PCA(n_components = 10)\n",
    "        pipe = make_pipeline(pca, model)\n",
    "    else:\n",
    "        pipe = make_pipeline(model)\n",
    "    grid = GridSearchCV(pipe,param_grid)\n",
    "    grid.fit(xtrain, ytrain)\n",
    "    grid.best_params_\n",
    "    accuracy = grid.score(xtrain, ytrain)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

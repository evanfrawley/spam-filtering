{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import mailbox\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# to make prior\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# to run models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing emails into a csv file, uses cleanMe() to remove unwanted data\n",
    "\n",
    "\n",
    "def createCSV(name, mbox, mes_type):\n",
    "    #createing SPAM csv\n",
    "    writer = csv.writer(open(name, \"w\", encoding=\"utf-8\"))\n",
    "    writer.writerow(['Subject', 'From', 'Date', 'Message-ID', 'Payload', 'Type'])\n",
    "    for message in mailbox.mbox(mbox):\n",
    "\n",
    "        if message.is_multipart():\n",
    "            content = ''.join(str(part.get_payload(decode=True)) for part in message.get_payload())\n",
    "            content = cleanMe(content)\n",
    "        else:\n",
    "            content = str(message.get_payload(decode=True))\n",
    "            content = cleanMe(content)\n",
    "        content = content.split()\n",
    "        print(content)\n",
    "        writer.writerow([message['subject'], message['from'], message['date'], message['Message-Id'], content, mes_type])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning emails, removing stop words\n",
    "## getting stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "## cleanMe: cleans body of email, removing javascript, css, blanklines, escape characters, links\n",
    "## and stop words. Also changes everything to lowercase\n",
    "\n",
    "def cleanMe(html):\n",
    "    soup = bs(html,\"lxml\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n '.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\t', ' ').replace(\"\\n\", ' ').replace('\\\\b', ' ')\n",
    "    # gets rid of escape characters\n",
    "    text = bytes(text, \"utf-8\").decode(\"unicode_escape\")\n",
    "    # gets rid of links \n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = \" \".join(re.findall(r'\\s+[a-zA-Z]+\\s+', text))\n",
    "    # make text lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove stop words before csv was written\n",
    "    text = word_tokenize(text)\n",
    "    resultwords  = [word for word in text if word not in stop_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writes CSV calling createCSV\n",
    "#createCSV('./data/spam_t1.csv', './data/Spam.mbox', 'Spam')\n",
    "#createCSV('./data/inbox_t1.csv', './data/Inbox.mbox', 'Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam shape:  (707, 6)\n",
      "ham shape:  (16238, 6)\n",
      "ham new shape:  (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "# importing CSV into dataframe\n",
    "spam = pd.read_csv(\"./data/spam_t1.csv\")\n",
    "ham = pd.read_csv(\"./data/inbox_t1.csv\")\n",
    "print('spam shape: ', spam.shape)\n",
    "print('ham shape: ', ham.shape)\n",
    "\n",
    "# ham has two years worth of emails, index starting from today and \n",
    "# ending way back to 2016. Trimming dataset to be more managable\n",
    "ham = ham[0 : 1000]\n",
    "print('ham new shape: ', ham.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getPrior takes data and result and returns the probablity of a single word \n",
    "# in the labeled email (hame or spam): columns are words, rows are emails\n",
    "vec = TfidfVectorizer()\n",
    "def getPrior(data, result):\n",
    "    X = vec.fit_transform(data)\n",
    "    prior = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "    prior['RESULT'] = result\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 == ham, 1 == spam\n",
    "spam_prior = getPrior(spam.Payload, 0)\n",
    "ham_prior = getPrior(ham.Payload, 1)\n",
    "\n",
    "# concat ham and spam prior to make prior feature set\n",
    "# replace NaN with zero because it means that spam did not have that word, or spam did not have that word\n",
    "frames = [spam_prior, ham_prior]\n",
    "prior = pd.concat(frames).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split target and data\n",
    "target = prior.RESULT\n",
    "data = prior.drop([\"RESULT\"], axis = 1)\n",
    "\n",
    "# spliting train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for MultinomialNB:\n",
      " [[122 118]\n",
      " [  2 322]]\n",
      "Confusion Matrix for SVC:\n",
      " [[214  26]\n",
      " [ 33 291]]\n",
      "Confusion Matrix for RandomForest:\n",
      " [[213  27]\n",
      " [ 24 300]]\n"
     ]
    }
   ],
   "source": [
    "# running Multinomial Naive Bayse, Random Forest, Linear SVC\n",
    "# without cross validation\n",
    "\n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "model3 = RandomForestClassifier()\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "# for extra credit :)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "result1 = model1.predict(X_test)\n",
    "result2 = model2.predict(X_test)\n",
    "result3 = model3.predict(X_test)\n",
    "\n",
    "# cheking results without cross validation\n",
    "\n",
    "print(\"Confusion Matrix for MultinomialNB:\\n\", confusion_matrix(y_test,result1))\n",
    "print(\"Confusion Matrix for SVC:\\n\", confusion_matrix(y_test,result2))\n",
    "print(\"Confusion Matrix for RandomForest:\\n\", confusion_matrix(y_test, result3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating parameters for pipeline for cross validation: \n",
    "# random forest\n",
    "pg_3 = {'randomforestclassifier__n_estimators': np.arange(1,10)}\n",
    "\n",
    "# multinomial Niave Bayse\n",
    "pg_1 = {'multinomialnb__alpha': np.arange(.5,1.5),\n",
    "        'multinomialnb__fit_prior': [True, False]}\n",
    "# svc classifier\n",
    "pg_2 = {'linearsvc__dual': [True, False],\n",
    "        'linearsvc__fit_intercept': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline using cross validation GridSearchCV to find best parameters \n",
    "def run_model(model, param_grid, xtrain, ytrain, name, do_pca = False):\n",
    "    scaler = MinMaxScaler()\n",
    "    if(do_pca == True):\n",
    "        pca = PCA(n_components = 10)\n",
    "        pipe = make_pipeline(pca, model)\n",
    "    else:\n",
    "        pipe = make_pipeline(model)\n",
    "    grid = GridSearchCV(pipe,param_grid)\n",
    "    grid.fit(xtrain, ytrain)\n",
    "    grid.best_params_\n",
    "    accuracy = grid.score(xtrain, ytrain)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\", name)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 99.04% Random Forest\n",
      "In-sample accuracy: 95.19% Multinomial NB\n",
      "In-sample accuracy: 98.86% Linear SVC\n"
     ]
    }
   ],
   "source": [
    "grid3 = run_model(model3, pg_3, X_train, y_train, \"Random Forest\")\n",
    "grid1 = run_model(model1, pg_1, X_train, y_train, \"Multinomial NB\")\n",
    "grid2 = run_model(model2, pg_2, X_train, y_train, \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[212  28]\n",
      " [ 26 298]]\n",
      "[[214  26]\n",
      " [ 33 291]]\n",
      "[[183  57]\n",
      " [ 23 301]]\n"
     ]
    }
   ],
   "source": [
    "# pipeline REALLY helps naive Bayse, however, I think it is overfitting \n",
    "# in linearCSV and Random Forest\n",
    "preds3 = grid3.predict(X_test)\n",
    "preds2 = grid2.predict(X_test)\n",
    "preds1 = grid1.predict(X_test)\n",
    "matrix3 = confusion_matrix(y_test, preds3)\n",
    "print(matrix3)\n",
    "matrix2 = confusion_matrix(y_test, preds2)\n",
    "print(matrix2)\n",
    "matrix1 = confusion_matrix(y_test, preds1)\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
